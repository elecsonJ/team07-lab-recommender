import os
import json
import numpy as np
from dotenv import load_dotenv
from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain.schema import Document
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import argparse
from typing import Dict, List, Any
from dataclasses import dataclass, field

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

@dataclass
class ConversationHistory:
    """ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬ í´ë˜ìŠ¤"""
    queries: List[str] = field(default_factory=list)
    responses: List[str] = field(default_factory=list)
    retrieved_docs: List[List[Document]] = field(default_factory=list)
    
    def add_turn(self, query: str, response: str, docs: List[Document] = None):
        self.queries.append(query)
        self.responses.append(response)
        if docs:
            self.retrieved_docs.append(docs)
    
    def get_context(self, last_n: int = 3) -> str:
        """ìµœê·¼ nê°œì˜ ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
        context = []
        start_idx = max(0, len(self.queries) - last_n)
        
        for i in range(start_idx, len(self.queries)):
            context.append(f"Q: {self.queries[i]}")
            if i < len(self.responses):
                context.append(f"A: {self.responses[i][:200]}...")  # ì‘ë‹µì€ 200ìë¡œ ì œí•œ
        
        return "\n".join(context)
    
    def clear(self):
        """íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"""
        self.queries.clear()
        self.responses.clear()
        self.retrieved_docs.clear()

class LabRecommenderRAG:
    def __init__(self, data_path, vector_store_path="./vector_store"):
        self.data_path = data_path
        self.vector_store_path = vector_store_path
        
        # Azure OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embeddings = AzureOpenAIEmbeddings(
            model="text-embedding-3-small",
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("OPENAI_API_KEY"),
            api_version=os.getenv("OPENAI_API_VERSION"),
            dimensions=1536
        )
        
        # Azure OpenAI LLM ëª¨ë¸ ì´ˆê¸°í™”
        self.llm = AzureChatOpenAI(
            model="gpt-4o-mini",
            azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
            api_key=os.getenv("OPENAI_API_KEY"),
            api_version=os.getenv("OPENAI_API_VERSION"),
            temperature=0.3
        )
        
        self.vector_store = None
        self.qa_chain = None
        self.conversation_history = ConversationHistory()
        
    def load_and_process_data(self):
        """êµìˆ˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  Document ê°ì²´ë¡œ ë³€í™˜"""
        with open(self.data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        documents = []
        for professor in data['êµìˆ˜ì§„']:
            # ì „ì²´ êµìˆ˜ ì •ë³´ë¥¼ í¬í•¨í•œ ìƒì„¸ í…ìŠ¤íŠ¸ ìƒì„±
            prof_info = f"""
=== ê¸°ë³¸ ì •ë³´ ===
êµìˆ˜ëª…: {professor['ê¸°ë³¸ì •ë³´']['êµìˆ˜ì´ë¦„']}
ëŒ€í•™ëª…: {professor['ê¸°ë³¸ì •ë³´'].get('ëŒ€í•™ëª…', '')}
í•™ê³¼ëª…: {professor['ê¸°ë³¸ì •ë³´'].get('í•™ê³¼ëª…', '')}
ì—°êµ¬ì‹¤: {professor['ì—°êµ¬ì‹¤']['ì—°êµ¬ì‹¤ëª…']}
ì—°êµ¬ë¶„ì•¼: {professor['ì—°êµ¬ë¶„ì•¼']['í‚¤ì›Œë“œ']}
ì—°êµ¬ë¶„ì•¼ ì„¤ëª…: {professor['ì—°êµ¬ë¶„ì•¼']['ì„¤ëª…']}
ì´ë©”ì¼: {professor['ê¸°ë³¸ì •ë³´']['ì´ë©”ì¼']}
ì „í™”ë²ˆí˜¸: {professor['ê¸°ë³¸ì •ë³´']['ì „í™”ë²ˆí˜¸']}
í•™ìœ„: {professor['ê¸°ë³¸ì •ë³´']['í•™ìœ„']}
ì—°êµ¬ì‹¤ ì›¹ì‚¬ì´íŠ¸: {professor['ì—°êµ¬ì‹¤'].get('ì—°êµ¬ì‹¤ì›¹ì‚¬ì´íŠ¸', '')}

=== ì—°êµ¬ ìƒì„¸ ì •ë³´ ===
ì—°êµ¬ì£¼ì œ:
{chr(10).join(professor['ì—°êµ¬ì£¼ì œ']) if professor['ì—°êµ¬ì£¼ì œ'] else 'ì •ë³´ ì—†ìŒ'}

ê¸°ìˆ  ë° ë°©ë²•:
{chr(10).join(professor['ê¸°ìˆ ë°ë°©ë²•']) if professor['ê¸°ìˆ ë°ë°©ë²•'] else 'ì •ë³´ ì—†ìŒ'}

í•™ë ¥ ë° ê²½ë ¥:
{chr(10).join(professor['í•™ë ¥ê²½ë ¥']) if professor['í•™ë ¥ê²½ë ¥'] else 'ì •ë³´ ì—†ìŒ'}

ì£¼ìš” ë…¼ë¬¸:
{chr(10).join(professor['ë…¼ë¬¸']) if professor['ë…¼ë¬¸'] else 'ì •ë³´ ì—†ìŒ'}

í•™ìƒì§€ë„ íŠ¹ì§•:
{professor['í•™ìƒì§€ë„'].get('íŠ¹ì§•', 'ì •ë³´ ì—†ìŒ')}

í•™ìƒ ì§„ë¡œ:
{professor['í•™ìƒì§€ë„'].get('ì§„ë¡œ', 'ì •ë³´ ì—†ìŒ')}
            """.strip()
            
            # ë©”íƒ€ë°ì´í„° ì„¤ì •
            metadata = {
                "professor_name": professor['ê¸°ë³¸ì •ë³´']['êµìˆ˜ì´ë¦„'],
                "university": professor['ê¸°ë³¸ì •ë³´'].get('ëŒ€í•™ëª…', ''),
                "department": professor['ê¸°ë³¸ì •ë³´'].get('í•™ê³¼ëª…', ''),
                "lab_name": professor['ì—°êµ¬ì‹¤']['ì—°êµ¬ì‹¤ëª…'],
                "email": professor['ê¸°ë³¸ì •ë³´']['ì´ë©”ì¼'],
                "phone": professor['ê¸°ë³¸ì •ë³´']['ì „í™”ë²ˆí˜¸'],
                "keywords": professor['ì—°êµ¬ë¶„ì•¼']['í‚¤ì›Œë“œ']
            }
            
            documents.append(Document(
                page_content=prof_info,
                metadata=metadata
            ))
        
        return documents
    
    def create_vector_store(self):
        """ë²¡í„° ì €ì¥ì†Œ ìƒì„±"""
        print("êµìˆ˜ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        documents = self.load_and_process_data()
        
        print("ë²¡í„° ì„ë² ë”©ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        self.vector_store = FAISS.from_documents(
            documents=documents,
            embedding=self.embeddings
        )
        # FAISS ì¸ë±ìŠ¤ ì €ì¥
        self.vector_store.save_local(self.vector_store_path)
        print(f"ë²¡í„° ì €ì¥ì†Œê°€ {self.vector_store_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def load_vector_store(self):
        """ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ"""
        try:
            self.vector_store = FAISS.load_local(
                self.vector_store_path,
                embeddings=self.embeddings,
                allow_dangerous_deserialization=True
            )
            print("ê¸°ì¡´ ë²¡í„° ì €ì¥ì†Œë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            print(f"ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def contains_professor_name(self, query: str) -> bool:
        """ì§ˆë¬¸ì— êµìˆ˜ëª…ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        professor_names = [
            "ê°•ê±´ìš±", "ê°•í™ê¸°", "êµ¬ìë¡", "ê¹€ë™í˜„", "ê¹€ëª…í™˜", "ê¹€ì„±ì¤€", "ê¹€ì¢…ì¼", "ê¹€í˜„ì œ", "ê¹€í˜„ì§„",
            "ë°•ìƒë¯¼", "ë°•ì„±ì¤€", "ë°•ìˆ˜ê²½", "ë°•ì •ê·œ", "ë°©ì˜ˆì§€", "ì„œì¸ì„", "ì„±ì§€í˜œ", "ì‹ í˜„ë¬´", "ì—¬ì„ ì£¼",
            "ì´ë¯¼ì¬", "ì´ìš©ì„", "ì´ì§€ì—°", "ì´ì§„êµ¬", "ì´ì°½í•œ", "ì´ì² í™˜", "ì¡°ì„±ì—½", "ì¡°ì£¼ì—°", "ìµœê²½í˜¸",
            "ìµœë¯¼í˜¸", "ìµœì€ì˜", "ìµœí˜•ì§„", "ì‹ ì •í™˜"
        ]
        for name in professor_names:
            if name in query:
                return True
        return False
    
    def can_answer_with_previous(self, query: str) -> bool:
        """ì´ì „ ê²€ìƒ‰ ê²°ê³¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        if not self.conversation_history.retrieved_docs:
            return False
        
        # ë‹¨ìˆœí•œ í›„ì† ì§ˆë¬¸ íŒ¨í„´ í™•ì¸
        followup_patterns = ["ê·¸ ì¤‘ì—ì„œ", "ë” ìì„¸íˆ", "ì¶”ê°€ë¡œ", "ê·¸ëŸ°ë°", "ë˜", "ê·¸ë¦¬ê³ "]
        return any(pattern in query for pattern in followup_patterns)
    
    def is_research_related(self, query: str) -> bool:
        """ì—°êµ¬ë¶„ì•¼ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸"""
        research_keywords = [
            "ì—°êµ¬", "AI", "ì¸ê³µì§€ëŠ¥", "ë¨¸ì‹ ëŸ¬ë‹", "ë°”ì´ì˜¤", "ì˜ë£Œ", "ìƒëª…ê³¼í•™", "ë¶„ì", "ì„¸í¬",
            "ìœ ì „", "ë©´ì—­", "ì˜ìƒ", "ì‹ ê²½", "ë‡Œ", "ì•”", "ì¢…ì–‘", "ì¹˜ë£Œ", "ì§„ë‹¨", "ì•½ë¬¼",
            "ì‹¤í—˜ì‹¤", "ì—°êµ¬ì‹¤", "êµìˆ˜", "ì¶”ì²œ", "ê´€ì‹¬", "í•˜ê³ ì‹¶ë‹¤", "ë°°ìš°ê³ ì‹¶ë‹¤"
        ]
        return any(keyword in query for keyword in research_keywords)
    
    def classify_query(self, new_query: str) -> Dict[str, Any]:
        """ê°œì„ ëœ ì§ˆë¬¸ ë¶„ë¥˜ ì‹œìŠ¤í…œ"""
        # 1. êµìˆ˜ëª… ì–¸ê¸‰ ì²´í¬
        if self.contains_professor_name(new_query):
            return {"type": "professor_detail", "reason": "íŠ¹ì • êµìˆ˜ ì–¸ê¸‰"}
        
        # 2. ì´ì „ ê²°ê³¼ë¡œ ë‹µë³€ ê°€ëŠ¥í•œì§€ ì²´í¬
        if self.can_answer_with_previous(new_query):
            return {"type": "refine_previous", "reason": "ì´ì „ ê²°ê³¼ í™œìš© ê°€ëŠ¥"}
        
        # 3. ì—°êµ¬ë¶„ì•¼ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ ì²´í¬
        if self.is_research_related(new_query):
            return {"type": "new_search", "reason": "ìƒˆë¡œìš´ ì—°êµ¬ë¶„ì•¼ ê²€ìƒ‰"}
        
        # 4. ë‚˜ë¨¸ì§€ëŠ” ì¼ë°˜ ì§ˆë¬¸
        return {"type": "general_info", "reason": "ëŒ€í•™ì› ì¼ë°˜ ì •ë³´"}
    
    def setup_qa_chain(self, k=5):
        """RAG QA ì²´ì¸ ì„¤ì •"""
        if self.vector_store is None:
            raise ValueError("ë²¡í„° ì €ì¥ì†Œê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # MMR ê²€ìƒ‰ê¸° ì„¤ì • (Maximum Marginal Relevance)
        retriever = self.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": k,
                "fetch_k": k*2,  # MMRì„ ìœ„í•´ ë” ë§ì€ í›„ë³´ë¥¼ ê°€ì ¸ì˜´
                "lambda_mult": 0.5  # ë‹¤ì–‘ì„±ê³¼ ê´€ë ¨ì„±ì˜ ê· í˜• ì¡°ì ˆ (0~1)
            }
        )
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        prompt_template = """ë‹¤ìŒì€ ëŒ€í•™ì› êµìˆ˜ì§„ì˜ ìƒì„¸ ì •ë³´ì…ë‹ˆë‹¤. í•™ìƒì˜ ì§ˆë¬¸ì— ê¸°ë°˜í•˜ì—¬ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•´ì£¼ì„¸ìš”.

êµìˆ˜ì§„ ì •ë³´:
{context}

í•™ìƒì˜ ì§ˆë¬¸: {question}

ë‹µë³€ ê°€ì´ë“œë¼ì¸:

**ì´ˆê¸° ì—°êµ¬ì‹¤ ì¶”ì²œì˜ ê²½ìš°:**
1. í•™ìƒì˜ ê´€ì‹¬ ë¶„ì•¼ì™€ ê°€ì¥ ìœ ì‚¬í•œ ì—°êµ¬ ë¶„ì•¼ë¥¼ ê°€ì§„ êµìˆ˜ë¥¼ 2-3ëª… ì¶”ì²œ
2. ê° êµìˆ˜ì— ëŒ€í•´ ê°„ëµí•˜ê²Œ ë‹¤ìŒ ì •ë³´ë§Œ ì œì‹œ:
   - êµìˆ˜ëª…ê³¼ ì—°êµ¬ì‹¤ëª…
   - ëŒ€í•™ëª…ê³¼ í•™ê³¼ëª… (ì„œìš¸ëŒ€í•™êµ ì˜ê³¼ëŒ€í•™)
   - í•µì‹¬ ì—°êµ¬ë¶„ì•¼ (1-2ì¤„ ìš”ì•½)
   - ì¶”ì²œ ì´ìœ  (1-2ì¤„)
3. "ë” ìì„¸í•œ ì •ë³´ê°€ ê¶ê¸ˆí•˜ì‹œë©´ íŠ¹ì • êµìˆ˜ë‹˜ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"ë¼ê³  ì•ˆë‚´

**íŠ¹ì • êµìˆ˜ì— ëŒ€í•œ ì„¸ë¶€ ì§ˆë¬¸ì˜ ê²½ìš°:**
1. í•´ë‹¹ êµìˆ˜ì˜ ìƒì„¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ êµ¬ì²´ì ìœ¼ë¡œ ë‹µë³€
2. ë…¼ë¬¸ ì œëª© í•´ì„, ì—°êµ¬ íŠ¸ë Œë“œ ë¶„ì„, í•™ë ¥ ë°°ê²½ ë“± ì‹¬í™” ì •ë³´ ì œê³µ
3. ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¼ ì ì ˆí•œ ì •ë³´ ì„ ë³„ (ë…¼ë¬¸/ì—°êµ¬ì£¼ì œ/í•™ë ¥ ë“±)

**ì¼ë°˜ ì§ˆë¬¸ì˜ ê²½ìš°:**
1. ëŒ€í•™ì› ìƒí™œ, ì…í•™ ì ˆì°¨ ë“±ì— ëŒ€í•´ ì¼ë°˜ì ì¸ ì¡°ì–¸ ì œê³µ

í•œêµ­ì–´ë¡œ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ë‹µë³€:"""

        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # QA ì²´ì¸ ìƒì„±
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": PROMPT},
            return_source_documents=True
        )
    
    def get_recommendation(self, user_query):
        """ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì—°êµ¬ì‹¤ ì¶”ì²œ (êµ¬ë²„ì „ í˜¸í™˜ìš©)"""
        if self.qa_chain is None:
            raise ValueError("QA ì²´ì¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print("\nğŸ” ê´€ë ¨ ì—°êµ¬ì‹¤ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
        
        result = self.qa_chain.invoke({"query": user_query})
        
        print("\n" + "="*60)
        print("ğŸ¯ ì—°êµ¬ì‹¤ ì¶”ì²œ ê²°ê³¼")
        print("="*60)
        print(result["result"])
        
        return result
    
    def process_new_search(self, user_query: str) -> Dict[str, Any]:
        """ìƒˆë¡œìš´ ê²€ìƒ‰ ì²˜ë¦¬ - ê°„ëµí•œ ì¶”ì²œ ëª¨ë“œ"""
        print("\nğŸ” ìƒˆë¡œìš´ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        result = self.qa_chain.invoke({"query": user_query})
        return result
    
    def process_refine_previous(self, user_query: str) -> Dict[str, Any]:
        """ì´ì „ ê²°ê³¼ ë‚´ì—ì„œ ì¬ê²€ìƒ‰"""
        if not self.conversation_history.retrieved_docs:
            # ì´ì „ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ìƒˆ ê²€ìƒ‰
            return self.process_new_search(user_query)
        
        print("\nğŸ”„ ì´ì „ ì¶”ì²œ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤...")
        
        # ì´ì „ ì¶”ì²œ êµìˆ˜ ì •ë³´ë§Œ ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
        previous_docs = self.conversation_history.retrieved_docs[-1]
        context_text = "\n\n".join([doc.page_content for doc in previous_docs[:5]])
        
        refined_prompt = f"""
ì´ì „ì— ì¶”ì²œí•œ êµìˆ˜ì§„ ì •ë³´:
{context_text}

í•™ìƒì˜ ì¶”ê°€ ì§ˆë¬¸: {user_query}

ìœ„ êµìˆ˜ì§„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ê°€ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        response = self.llm.invoke(refined_prompt)
        return {"result": response.content, "source_documents": previous_docs}
    
    def process_professor_detail(self, user_query: str) -> Dict[str, Any]:
        """íŠ¹ì • êµìˆ˜ ìƒì„¸ ì •ë³´ ì²˜ë¦¬"""
        print("\nğŸ‘¨â€ğŸ« íŠ¹ì • êµìˆ˜ë‹˜ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤...")
        result = self.qa_chain.invoke({"query": user_query})
        return result
    
    def process_general_info(self, user_query: str) -> Dict[str, Any]:
        """ì¼ë°˜ ì •ë³´ ì²˜ë¦¬ (RAG ì—†ì´)"""
        print("\nğŸ’¬ ëŒ€í•™ì› ì¼ë°˜ ì •ë³´ì— ë‹µë³€í•©ë‹ˆë‹¤...")
        
        general_prompt = f"""
ëŒ€í•™ì› ì¼ë°˜ ì§ˆë¬¸: {user_query}

ë‹¤ìŒê³¼ ê°™ì€ ì£¼ì œì— ëŒ€í•´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- ëŒ€í•™ì› ì…í•™ ì ˆì°¨ ë° ì¤€ë¹„ì‚¬í•­
- ì—°êµ¬ì‹¤ ìƒí™œ ë° ì—°êµ¬ ê³¼ì •
- ì§€ì› ìê²© ë° ìš”êµ¬ì‚¬í•­
- ëŒ€í•™ì›ìƒìœ¼ë¡œì„œì˜ ì¼ë°˜ì ì¸ ì¡°ì–¸

ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.
"""
        
        response = self.llm.invoke(general_prompt)
        return {"result": response.content, "source_documents": []}
    
    def process_query(self, user_query: str) -> str:
        """ì§ˆë¬¸ ë¶„ë¥˜ í›„ ì ì ˆí•œ ì²˜ë¦¬"""
        # ì§ˆë¬¸ ë¶„ë¥˜
        classification = self.classify_query(user_query)
        query_type = classification.get("type", "new_search")
        reason = classification.get("reason", "")
        
        print(f"\nğŸ¤– ì§ˆë¬¸ ë¶„ë¥˜: {query_type}")
        print(f"   ì´ìœ : {reason}")
        
        # ë¶„ë¥˜ì— ë”°ë¥¸ ì²˜ë¦¬
        if query_type == "new_search":
            result = self.process_new_search(user_query)
        elif query_type == "refine_previous":
            result = self.process_refine_previous(user_query)
        elif query_type == "professor_detail":
            result = self.process_professor_detail(user_query)
        elif query_type == "general_info":
            result = self.process_general_info(user_query)
        else:
            result = self.process_new_search(user_query)
        
        # íˆìŠ¤í† ë¦¬ì— ì €ì¥
        response_text = result["result"]
        source_docs = result.get("source_documents", [])
        self.conversation_history.add_turn(user_query, response_text, source_docs)
        
        return response_text

def main():
    parser = argparse.ArgumentParser(description='ëŒ€í•™ì› ì—°êµ¬ì‹¤ ì¶”ì²œ AI')
    parser.add_argument('--rebuild', action='store_true', 
                       help='ë²¡í„° ì €ì¥ì†Œë¥¼ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤')
    parser.add_argument('--k', type=int, default=5,
                       help='ê²€ìƒ‰í•  ì—°êµ¬ì‹¤ ìˆ˜ (ê¸°ë³¸ê°’: 5)')
    
    args = parser.parse_args()
    
    # ë°ì´í„° ê²½ë¡œ
    data_path = "professors_final_complete.json"
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag_system = LabRecommenderRAG(data_path)
    
    # ë²¡í„° ì €ì¥ì†Œ ì„¤ì •
    if args.rebuild or not rag_system.load_vector_store():
        rag_system.create_vector_store()
    
    # QA ì²´ì¸ ì„¤ì •
    rag_system.setup_qa_chain(k=args.k)
    
    print("\nğŸ“ ëŒ€í•™ì› ì—°êµ¬ì‹¤ ì¶”ì²œ AIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
    print("ê´€ì‹¬ìˆëŠ” ì—°êµ¬ ë¶„ì•¼ë‚˜ ì£¼ì œë¥¼ ììœ ë¡­ê²Œ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("ëŒ€í™”ë¥¼ ìƒˆë¡œ ì‹œì‘í•˜ë ¤ë©´ 'clear' ë˜ëŠ” 'reset'ì„ ì…ë ¥í•˜ì„¸ìš”.\n")
    
    # ì²« ì§ˆë¬¸ ì—¬ë¶€ ì¶”ì 
    is_first_question = True
    
    while True:
        try:
            # ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ ê°œì„ 
            if is_first_question:
                prompt = "\nğŸ’­ ì–´ë–¤ ì—°êµ¬ ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹ ê°€ìš”? >> "
            else:
                prompt = "\nğŸ’¬ ì¶”ê°€ ì§ˆë¬¸ì´ ìˆìœ¼ì‹ ê°€ìš”? >> "
            
            user_input = input(prompt).strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'ë']:
                print("\nğŸ‘‹ ëŒ€í•™ì› ì—°êµ¬ì‹¤ ì¶”ì²œ AIë¥¼ ì´ìš©í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤!")
                break
            
            if user_input.lower() in ['clear', 'reset', 'ì´ˆê¸°í™”', 'ìƒˆë¡œì‹œì‘']:
                rag_system.conversation_history.clear()
                print("\nğŸ”„ ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
                is_first_question = True
                continue
            
            if not user_input:
                print("â— ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            # ì§ˆë¬¸ ì²˜ë¦¬ ë° ì‘ë‹µ
            print("\n" + "="*60)
            response = rag_system.process_query(user_input)
            print("="*60)
            print(response)
            print("="*60)
            
            is_first_question = False
            
        except EOFError:
            print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            print("ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            continue

if __name__ == "__main__":
    main()